### Section 1: Section 1: Abstract
Content: Content: ALBEFbecause lunch is more interesting than job and even tasty ..sarcasticsarcastic text2 modalities provide redundant informationALBEFhappy spring ! loving all the blossoming flowers happeni
...
ng here !
...
 so beautiful !not sarcasticnot sarcastic text2 modalities interact to provide new information

### Section 2: Section 2: M M oE
Content: Content: Figure 1: A single model cannot handle all types ofmultimodal interactions well for hard multimodalprediction tasks. For example, to predict sarcasm, AL-BEF can have ‚àº89% F1 when modalities c
...
ontain re
...
dun-dant information (e.g., both the text and the image aresarcastic), but drops to ‚àº24% F1 when there is synergybetween modalities (e.g., the image shows a cold winterscene and the text says it is a happy spring, indicatingthe user‚Äôs sarcastic intent about the weather).

### Section 3: Section 3: 1Introduction
Content: Content: arXiv:2311.09580v3  [cs.CL]  25 Sep 2024and Beer, 2010; Liang et al., 2023a; Marsh andDomas White, 2003). Instead, it might hinge onunique details from either modality (e.g. detectinglaughter
...
 from som
...
eone not observed) or the resultof synergistic fusion of both modalities, producinginsights absent when either modality is consideredin isolation (e.g., sarcasm and humor discernedfrom incongruent speech and gestures). Synergy isparticularly interesting because it often arises whenthe predictions from different modalities are contra-dicting, or incongruent with one another (Bateman,2014; Kruk et al., 2019; Zhang et al., 2018).Recent advances in the design and pretraining ofvision-language models have enabled significantprogress in capturing the correspondences betweenimages and text (Zhu et al., 2023; Li et al., 2023;Liu et al., 2023). These models have seen suc-cesses in image captioning (Xu et al., 2015), text-to-image generation (Saharia et al., 2022), multi-modal retrieval (Mithun et al., 2018), multimodalclassification (Li et al., 2021), and more. At itscore, these methods aim to capture overlaps in se-mantic content between images and text, makinga strong multi-view redundancy assumption (Tianet al., 2020; Liang et al., 2023b; Zbontar et al.,2021). However, redundancy is only one type ofinteraction seen between two modalities (Williams*Equal Contribution.The diversity of possible real-world multimodalinteractions poses a challenge to today‚Äôs multi-modal models. Empirically, we find that one singlemodel may not be the most suitable in capturing all1

### Section 4: Section 4: Multimodal Interactions
Content: Content: types of interaction at the same time. For example,models trained to learn the correspondences be-tween words and image regions (e.g., for retrieval)will struggle when there is unique informa
...
tion inon
...
e modality (Liang et al., 2023b; Winterbottomet al., 2020), or when the image and text providecontradicting information that must be contextual-ized together (Hessel et al., 2022). We show an ex-ample of this failure in Figure 1, where ALBEF (Liet al., 2021) can easily detect sarcasm when it ispresent in both modalities (redundancy), but failswhen the sarcastic intent arises from the synergis-tic fusion of both image and text. Quantitatively,ALBEF has a performance drop of up to 60% ondata with synergistic interactions compared withthose with redundancy interactions.To tackle this problem, we propose MMOE, byleveraging the key insight that different interactionsrequire different modeling paradigms. A naturalway to model these differences is to use a mixtureof multimodal experts with one specialized expertmodel for each interaction. Each expert model canbe specialized based on the unique training datathey see or a special training objective. Further-more, there is evidence that the brain also usesseparate expert regions during the multisensory in-tegration process, depending on the types of inputmodalities and multimodal contexts present duringperception (Stein et al., 2020). During inference onunseen data points at testing time, MMOE relies onspecific fusion methods to provide weights for eachexpert model, combine the output of each expertmodel, and obtain a final prediction.MMOE achieves new state-of-the-art results onone multimodal sarcasm detection dataset and onemultimodal humor detection dataset we tested on,MUStARD and URFunny. Moreover, we show thatour approach is easy to implement on various typesof models: fusion-based vision language modelslike ALBEF (Li et al., 2021), multimodal extendedlarge language models like BLIP2 (Li et al., 2023),and image-captioned large language models likeQwen2 (Yang et al., 2024a) all improve after addingMMOE on top of them. 1

### Section 5: Section 5: 2Related Work
Content: Content: We cover related work in quantifying and learningmultimodal interactions, as well as recent advancesin multimodal large language models, ensembling,and mixtures of experts.1Codebase and repro
...
duction g
...
uidance are available athttps://github.com/lwaekfjlk/mmoe

### Section 6: Section 6: Multimodal Language Models
Content: Content: 2Typeùë¶!ùë¶"ùë¶#‚àóùë¶! = ùê¥VisionModelAAARABBUùë¶#‚àó= ùêµOh, I'm so glad you asked AABit like that. You.ùë¶" = ùê¥STextModelOh, I'm so glad you asked it like that. You.ABCFigure 2: We classify one multimodal d
...
ataset in
...
to three subsets based on their multimodal interactions: (1)Redundancy (R), when both modalities provide the same prediction, (2) Uniqueness (U), when two modalities makedifferent predictions, of which one of them is correct, (3) Synergy (S), when the ground-truth multimodal labels donot agree with either of unimodal predictions. y1 represents the prediction based on vision modality, y2 representsthe prediction from text modality, and y‚àóm represents the ground-truth label. {A, B, C} represents classes.els during inference on unseen new data points. Wenow explain each of these three steps in detail.

### Section 7: Section 7: 3.1Categorizing Multimodal Interactions
Content: Content: Prior work has provided definitions of redundant,unique, and synergistic interactions using the lan-guage of information theory (Williams and Beer,2010; Liang et al., 2023a). However, estimat
...
inginform
...
ation theoretic measures can be challeng-ing for high-dimensional and continuous distribu-tions (P√©rez-Cruz, 2008). When these interactionscannot be exactly computed, they can be approxi-mately inferred by considering whether unimodalmodels trained on each modality agree or disagreewith each other. We can formalize the conceptof modality agreement and disagreement with adiscrepancy function as follows:

### Section 8: Section 8: Definition 1.
Content: Content: Œ¥(y1, y2) =(0,if y1 = y2,1,if y1 Ã∏= y2.We focus on multimodal prediction tasks: givenfeature vectors from two modalities with x1 andx2, our goal is to predict the label y using both x1and x2.
...
 Naturall
...
y, task-related information may becontained uniquely in one of the modalities, presentredundantly in both, or require synergistically com-bining of information from both modalities. Whileprior work has focused on designing a single multi-modal model for all data points in a task, our keyinsight is that each data point may exhibit a dif-ferent type of interaction and therefore require adifferent modeling approach. Our method, whichwe call MMOE, is a natural solution to this prob-lem in three steps (1) Categorizing: categorizingmultimodal interaction types in each data point forthe training set, (2) Training: training three expertmodels to master at each type of interactions (re-dundancy, uniqueness, and synergy), (3) Inference:dynamically ensembling the mixture of expert mod-The binary discrepancy function indicates thatmodalities agree with each other when Œ¥ = 0and modalities disagree with each other whenŒ¥ = 1. Combining them with multimodal predic-tions, gives us an intuitive guideline to categorizedata points based on three types of interactions:1. Redundancy: when both modalities agree withthe multimodal prediction, two modalities con-tain redundant information.3ùë¶"Encùë¶!ùë¶"ùë¶#VLMFuseFuseTextEncmodelmodelmodelmodelmodelmodelEncMLLMLLMTextCaptionLLMLLMTextInferenceApplicabilityTraining

### Section 9: Section 9: inference
Content: Content: 2. Uniqueness: when two modalities disagree andone of them is aligned with the multimodal pre-diction, two modalities contain unique informa-tion and one is dominant.3. Synergy: when the mult
...
imodal pr
...
ediction dis-agrees with both unimodal predictions so thereis synergistic information generated from twomodalities when predicting.With such intuitive guidelines above, we for-mally define the categorization process as follows:for all training data points. For vision-only pre-dictions, we utilize vision-language models likeCogVLM2 (Wang et al., 2023) to obtain them byproviding only the query and the image and makesure that generated answers are conditioned onlyon the vision-side information. To get text-onlypredictions, we use state-of-the-art language mod-els like Qwen2-72B-Instruct (Yang et al., 2024a)with the query and the language information so themodel answers are conditioned only on text for pre-diction. More information related to the collectionof unimodal labels is available in Appendix ¬ßF.3.2Training Expert Models for EachMultimodal Interaction Type

### Section 10: Section 10: Theorem 1.
Content: Content: ‚àÜ1,2(y1, y2, ym) = Œ¥(y1, ym) + Œ¥(y2, ym)where Œ¥(¬∑, ¬∑) denotes the discrepancy function be-tween two predictions. The categorization is thendescribed as follows:‚Ä¢ ‚àÜ1,2 = 0: Redundancy, i.e., y
...
1 = y2 = 
...
ym,‚Ä¢ ‚àÜ1,2 = 1: Uniqueness, i.e., y1 = ym Ã∏= y2 ory2 = ym Ã∏= y1,‚Ä¢ ‚àÜ1,2 = 2: Synergy, i.e., y1 Ã∏= ym and y2 Ã∏= ym.To illustrate the categorization rule, Figure 2shows an example. In practice, obtaining high-quality predictions can be challenging. Labels frommultimodal datasets, which are typically generatedby humans making multimodal predictions, can bedirectly used. Also, we obtain high-quality uni-modal predictions y1 and y2 via state-of-the-artfoundation models in the few-shot prompting styleGiven the categorization of multimodal datasetsinto subsets each with a similar interaction, this sec-tion describes how we use these interaction-specificsubsets to train interaction-specific expert models.Illustrated in Figure 3, there are three specializedmodels, which we term fr, fu, and fs for expertmodels of redundancy, uniqueness, and synergyrespectively. While these individual expert modelsshare the same format of inputs with image and textdata pairs, their learning outcomes can differ sig-nificantly due to the multimodal data distributionsthey are trained on.Overall, for expert model training, we collect allhigh-quality evidence of redundant interactions totrain a redundancy expert model fr. This processis repeated for unique and synergistic interactions,resulting in trained expert models fr, fu, and fs.Each expert is trained only on the subset of datapoints that maximally exhibit that interaction; thisspecialization enables experts to be performant atlearning that specific interaction. More technicaldetails about the training process of expert models4based on the frequency statistics of each interac-tion to weight each expert model and so on; seedetailed ablation studies on these fusion methodsin Section ¬ß6 and fusion model training detailsin Appendix ¬ßH. Using these inferred weights foreach expert model, we obtain a final predictionÀÜy = Pi=‚àà{r,u,s} wifi(x1, x2) as the output ofMMOE.

### Section 11: Section 11: 4Experiments
Content: Content: Our experiments are designed to evaluate the effec-tiveness of our method when applied to a diverseset of multimodal foundation model architecturesand multimodal prediction tasks.are further 
...
discussed
...
 in Appendix ¬ßG.We also note that it is possible to design inter-action expert models using different modeling ar-chitectures and training objectives based on inno-vations in multimodal machine learning. For exam-ple, it has been empirically demonstrated that latefusion models are more suitable when modalitiesare redundant (Gadzicki et al., 2020), and mod-els with expressive higher-order interactions (e.g.,polynomials and tensors) are suitable when thereis synergy between modalities (Hou et al., 2019).Moreover, multi-task training allows us to lever-age the power of scale and learn interaction expertmodels adaptable to multiple tasks simultaneously.We leave these explorations for our future work.

### Section 12: Section 12: 3.3Inference with Mixtures of Expert Models
Content: Content: We introduce the models and multimodal predic-tion tasks that we consider for experiments in thissection. More information related to experimentalsettings is available in Appendix ¬ßI.
...
...


### Section 13: Section 13: Model
Content: Content: 2. Multimodal-extended LLMs (MLLM) includesmodels like BLIP2 (Li et al., 2023) and FRO-MAGe (Koh et al., 2023). It starts with an im-age encoder and an LLM as the backbone of thearchitecture.
...
 Most sta
...
te-of-the-art models arebased on multimodal-extended LLMs.3. Image-captioned LLMs (LLM) convert imagesto text using an image captioning model anduses a text-only LLM like Qwen2 (Yang et al.,2024a) on the concatenation of captioned im-ages and text inputs. Examples include the So-cratic Model (Zeng et al., 2022) and the videounderstanding model (Zhang et al., 2023).The conclusion of Section ¬ß3.2 yields three expertmodels each suited for a certain type of multimodalinteractions. During inference on unseen test datapoints, we need to select one or more expert mod-els that are most suitable to get the final predic-tion. This is a challenge since the categorizationof data points during training (presented in Sec-tion ¬ß3.1) relies on the multimodal prediction ym,which we have during training but not during in-ference. Therefore, we need to design a methodto provide an accurate estimation of the potentialmultimodal interactions included in one data point.Our key assumption is that categorizing mul-timodal interactions within a data point is an es-sential sub-task that must be completed beforethe model can generate a final prediction. Themultimodal interaction type captures the informa-tion shift between unimodal and multimodal inputs.These interaction-type predictions emphasize moregeneral features compared to those needed for tasklabel prediction. Consequently, even if a multi-modal model struggles to accurately predict thetask label y‚àó, it may still be able to determine theinteraction type of the data point (e.g., whether twomodalities provide similar, distinct, or synergisticinformation). This distinction becomes particularlyrelevant when the prediction task involves regres-sion or classification with many classes.Therefore, we approximately categorize data dur-ing inference through a soft mixture of weights,defined as wr, wu, and ws over the three interac-tion types. These weights are inferred dynami-cally for each data point using a finetuned fusionmodel (e.g., BLIP2 in practice).We also testsimple model-free baselines like prior constants

### Section 14: Section 14: Multimodal prediction task
Content: Content: 5
...
...


### Section 15: Section 15: ModelAccF1
Content: Content: MulT‚Ä† (Tsai et al., 2019)-64.49LMF‚Ä† (Liu et al., 2018)-69.92LFDNNv1‚Ä† (Ding et al., 2022)-70.99ALBEF54.49¬±3.13 48.51¬±2.21ALBEF+MMOE54.49¬±2.85 51.95¬±2.81et al., 2023), and one humor detection t
...
ask, whic
...
his URFunny (Hasan et al., 2019). These tasks re-quire interaction learning to conduct prediction.Detailed information about dataset statistic infor-mation and their preprocessing methods are avail-able in Appendix ¬ßC and ¬ßD.

### Section 16: Section 16: MUStARD
Content: Content: BLIP253.75¬±9.33 62.65¬±2.67BLIP2+MMOE59.18¬±2.11 64.74¬±2.49Qwen2-0.5B54.59¬±4.35 58.17¬±0.86Qwen2-0.5B+MMOE49.06¬±3.00 59.77¬±0.35
...
...


### Section 17: Section 17: 70.69 72.34
Content: Content: MulT‚Ä† (Tsai et al., 2019)66.65-FDMER (Yang et al., 2022)70.43-
...
...


### Section 18: Section 18: 4.2Main Results
Content: Content: ALBEF66.77¬±0.86 68.67¬±0.18ALBEF+MMoE67.91¬±0.31 69.85¬±0.32
...
...


### Section 19: Section 19: Overall comparison with state-of-the-art
Content: Content: ALBEF81.79¬±0.24 79.33¬±0.79ALBEF+MMOE82.30¬±0.27 80.63¬±0.52MMSD2.0BLIP284.75¬±0.20 83.52¬±0.35BLIP2+MMOE84.82¬±0.30 83.38¬±0.36Qwen2-0.5B81.87¬±0.54 80.17¬±0.14Qwen2-0.5B+MMOE82.27¬±0.14 80.67¬±0.20Tab
...
le 1: MMO
...
E can beat state-of-the-art models forMUStARD and URFunny. It can be applied to any typeof model for improvement. The numbers in the tablerepresent the mean values from 3 runs with 3 seeds,with the corresponding standard variance provided. Fullresults can be found in Appendix ¬ßB. ‚Ä† indicates thatmodels utilize all audio, text, and vision informationprovided in the dataset while ours only utilizes text andvision information for prediction.

### Section 20: Section 20: 5Analysis
Content: Content: Based on these quantitative results, we further pro-vide a fine-grained analysis of our method. First,we examine the limitations of current multimodalmodels by presenting empirical evidence w
...
herea sin
...
gle model faces challenges in typical typesof interactions. We then explore whether special-ized multimodal interaction expert models excel intheir respective interaction types. Furthermore, weanalyze the scaling law of expert models and dis-cuss whether these expert models can be potentiallysmaller, in contrast to typically overparameterizedmodels. Lastly, we provide additional details onthe unimodal predictions and emphasize their im-portant role in the data categorization process.Improvement on various types of modelsWefirst compare the performance of 3 types of mod-els with and without MMOE on MUStARD dataset.As shown in Table 1, all models, including AL-BEF, BLIP2, and Qwen2, show improvements inF1 scores. Notably, Qwen2-1.5B achieves an in-crease of 6.96 points, establishing it as the state-of-the-art model on this task. Additionally, on theMMSD2.0 dataset, both ALBEF and Qwen2 demon-strate performance gains, while BLIP2 remainsrelatively unchanged. For the URFunny dataset, AL-BEF improves accuracy by 1.14 points, and BLIP2by 0.84 points, whereas Qwen2 experiences a slightdecline after applying MMOE. The performancedrop on URFunny may be due to the inability ofimage captioning models to provide useful descrip-tions relevant to humor detection from the TED talkvideos. As a result, text-based models like Qwen2might struggle to achieve further improvements.Furthermore, when comparing the performanceacross the three prediction tasks and three mod-els, we observe a general trend: incorporating theMMOE tends to provide more robust improve-ments on challenging datasets (e.g., MUStARD) andweaker models (e.g., ALBEF) with low F1 scores,which initially have lower performance. In contrast,the improvements are less pronounced on easierdatasets (e.g., MMSD2.0) or stronger models (e.g.,BLIP2), which already exhibit strong performance.6Performance Comparison by Interaction TypeùëÖeùëëùë¢ùëõùëëùëéùëõùëêùë¶ùëã" (Image) ùëã! (Text) 10088.7Yeah, I mean, it would be rude to 80.4ùëã" ùëã! them for us to RedundancyUniquenessSynergy8076.0leave now.66.1ùëå63.56054.5ùëàùëõùëñùëûùë¢ùëíùëõùëíùë†ùë†UniquenessSynergyF1 Score40.040They are attending a large event and everyone stands up to clap. They definitely should not leave 24.4at this point. It indicates sarcasm.2012.5synergy Information for sarcasm detectionMustardUrFunnyMMSD2.0

### Section 21: Section 21: Case study on synergy interaction
Content: Content: Figure 6: Multimodal models struggle with synergymuch more than redundancy and uniqueness. AL-BEF shows significantly lower performance on syner-gistic datapoints compared with redundancy and
...
 unique-n
...
ess that are categorized based on our method.RQ2. How do expert models perform oncorresponding multimodal interaction data?RQ1. What types of multimodal interaction docurrent models struggle with?While a single large multimodal model may strug-gle, MMOE leverages specialized expert modelsto handle each type of interaction. As shown in Ta-ble 2, these expert models for redundancy, unique-ness, and synergy outperform test data points withtheir corresponding interaction types.Notably,expert models for synergy and redundancy showthe most significant improvements in MMSD2.0:Qwen2-0.5B gains over 30 F1 points on synergy,and ALBEF improves by around 8 F1 points onredundancy. In contrast, expert models for unique-ness exhibit almost no change across differentmodel settings. This could be because data pointswith unique interactions are more prevalent inthe dataset compared to those with redundancy orsynergy (data points with uniqueness account foraround 61%). As a result, baseline models tendto focus on learning these features during training,leading to similar performance with expert models.RQ3. How small can expert models be?In Figure 6, we categorize all test data pointsbased on their corresponding interaction type us-ing the method mentioned in Section ¬ß3.1. Weobserve significant performance variations whenusing the same model to predict across datawith different interaction types. Across the threedatasets‚ÄîMUStARD, URFunny, and MMSD2.0 ‚Äîdatapoints with synergy interactions show markedlylower F1 scores compared to those with unique-ness interactions, with performance gaps of 27.5,9.0, and 51.6 for MUStARD, URFunny, and MMSD2.0,respectively. Also, data points with uniqueness in-teraction perform substantially worse than thosewith redundancy interaction, with gaps of 26.1,16.9, and 12.7 for three datasets. These trends arenot limited to ALBEF, as we observe similar pat-terns in BLIP2 and Qwen2, highlighting that datapoints with strong synergy interactions represent acommon challenge across all three types of models.To better understand why models struggle withsynergy-type interactions, we provide a case studyin Figure 7 that highlights such failure. In this ex-ample, both the visual input (people watching ashow and clapping) and the language input (theythink they should not leave) lack clear signals ofsarcasm individually. However, when combined,the synergized information (where "them" refers toa band or show and "now" refers to the beginningor ending point of that) reveals an evident sarcas-tic intent that is not present in the original visualor language cues. Despite large-scale pretraining,multimodal models struggle to capture such com-plex interactions between modalities accurately.It is well established that neural networks, givenenough parameters, are universal function approx-imators. Therefore, sufficiently large multimodalmodels should eventually be capable of learning allinteraction types. However, we hypothesize that ex-pert models can be smaller and benefit more fromMMOE. To explore the scaling law of MMOE, weconducted an empirical study using Qwen2 modelsof different sizes (0.5B, 1.5B, and 7B). We ob-served a linear relationship between model sizeand performance score when plotted on a log-scalex-axis, as shown in Figure 8.7of its size or baseline performance.

### Section 22: Section 22: ModelTrainingRUS
Content: Content: ALBEFw/o expert train88.7076.0224.39w/ expert train96.6676.3328.95RQ4. Is the improvement of MMOE primarilydriven by model ensembling?BLIP2w/o expert train96.8980.1620.56w/ expert train99.108
...
0.1648.98
...
Qwen2-0.5Bw/o expert train93.7176.1421.43w/ expert train96.5476.1653.66Table 2: Performance of expert models on MMSD2.0.Expert training based on the corresponding interactiontype improves the model‚Äôs ability to predict test datapoints of the same type.MMSD2.0 F1 of Different Sizes of Qwen2888786858483MMSD2.0 F1828180ModelBaselineMMoEMMoE OracleWe investigate whether the performance gains inMMOE are primarily driven by our proposed mul-timodal interaction-driven data categorization (intoredundancy, uniqueness, and synergy) instead ofsimple multiple model ensembling. To test thishypothesis, we conduct an ablation study usingthe URFunny dataset. In this experiment, we keptthe number of training data points for each expertmodel unchanged but replaced the correspondingdata points with the ones randomly sampled fromthe dataset. To eliminate any potential influenceintroduced by the different fusion methods duringinference, we calculate the cross-entropy loss fromthe three expert models with the smallest valuesand averaged the score on the whole dataset to as-sess the upper-bound performance of the mixturesof experts. The metric is defined as:790.5B1.5B7BQwen2 Model Size (Log Scale)NXCEmoe = 1Ni=1miny‚àà{yr,yu,ys} CE(y, y‚àó)(1)

### Section 23: Section 23: gains better improvement onsmaller models
Content: Content: where N represents the total number of the dataset,yr, yu, and ys represents the logits from expertmodels and y‚àórepresents the ground-truth labels.We show that for our multimodal interaction-
...
basedcate
...
gorization, the cross-entropy loss is 0.5853while for random sampling categorization, thecross-entropy loss is 0.6942 (18.59% increase com-pared with our proposed categorization). Addition-ally, the original single model baseline has a lossof 0.8070. It indicates that our methods help buildbetter models for the whole dataset.RQ5. What do unimodal predictions look like?When applying MMOE to the 7B model, its per-formance worsens compared to the single-modelbaseline. However, as the model size decreases, thebenefits of MMOE become increasingly significant.This scaling law suggests that MMOE is more ef-fective with smaller expert models with worse sin-gle model performance, which makes sense sincesmaller models typically struggle to handle multi-ple interaction types, and specialized expert mod-els can address this limitation more effectively bytraining on data with specific types of interactions.Additionally, we also include the oracle perfor-mance of MMOE when using an oracle router forclassifying interaction types in Figure 8. With sucha router, each data point is always directed to theappropriate expert model for inference. In this set-ting, the mixture of experts achieves significantlyhigher performance compared to baseline modelsand shows a steeper slope when scaling to largermodels. This finding suggests that the primarybottleneck of MMOE lies in training an accuraterouter to route data to the correct expert model foreach interaction type. Moreover, it highlights thata model‚Äôs imbalanced ability to handle differenttypes of multimodal interactions persists regardlessThe quality of unimodal partial labels is crucialfor accurate data categorization, as these labelsdirectly influence the categorization process. Asdiscussed in Section ¬ß3.1, we utilize state-of-the-art models to generate unimodal predictions forthe training set. Table 2 demonstrates that acrossall datasets‚Äîincluding MUStARD, URFunny, andMMSD2.0 ‚Äîthere is a clear bias toward the textmodality. Text-based predictions are 16 pointsmore accurate than those based on visual informa-tion. Moreover, predictions from the visual modal-ity exhibit significantly lower confidence comparedto those from the text-based modality, indicatingthat the visual side offers few reliable features formodel predictions.8

### Section 24: Section 24: AccF1ConfAccF1Conf
Content: Content: MUStARD 66.9665.450.9350.8764.720.57URFunny 67.8761.200.9750.3962.270.48MMSD2.0 66.8855.740.9749.5860.390.63prevent highly imbalanced label distributions afterdata categorization. Rebalancing
...
 helps av
...
oid train-ing collapse in expert models, especially synergyexpert models where the training data is few. Fur-ther details on data filtering and label rebalancingcan be found in Appendix F.3.

### Section 25: Section 25: 6.2Ablation study on expert model fusion
Content: Content: Table 3: Quality and confidence of unimodal pre-diction. Conf refers to the confidence of a prediction,calculated as the average of the maximum logits for thetokens "Yes" and "No" from the mo
...
del‚Äôs fin
...
al outputlogits over the entire vocabulary.

### Section 26: Section 26: Fusion MethodMUStARDURFunnyMMSD2.0
Content: Content: Baseline47.9068.8778.87Average fusion47.1669.1780.34Maximum fusion47.8469.5580.70Weighted fusion48.8669.3980.25Model-based fusion48.9770.2080.71
...
...


### Section 27: Section 27: 56.8973.3682.73
Content: Content: Table 4: Ablation study on various fusion methodson ALBEF. Baseline indicates the single model per-formance of ALBEF without fusion. Oracle refers tofusion performed on the test set that has 
...
been cate
...
go-rized using the same method applied to the training data.

### Section 28: Section 28: 6Ablation Study
Content: Content: We conduct ablation studies on technical details instages of categorizing and inference.
...
...


### Section 29: Section 29: 6.1Ablation study on data categorization
Content: Content: We also explore how different fusion strategies forcombining multiple expert models impact perfor-mance. As mentioned in Section ¬ß5, fusion meth-ods play a significant role during inference, 
...
sug-gesti
...
ng that each expert model focuses on differ-ent aspects of multimodal information, and mix-ing them up simply cannot take full use of theirprediction ability. The common fusion methodswe consider include: (1) Average Fusion: wherewe simply average the softmaxed logits from theexpert models to produce the final result. (2) Max-imum Fusion: where we select the highest logitsfrom all the expert models as the final prediction.(3) Weighted Fusion: for each dataset, we assign afixed weight to each expert model, with the weightsreflecting the proportion of each interaction typewithin the whole dataset. (4) Model-based Fusion:where we use a BLIP2-based classifier trained todistinguish between redundancy, uniqueness, andsynergy. This classifier dynamically adjusts theweights for each expert model for each data pointaccordingly. Based on Table 4, we find that model-based fusion generally provides the most signifi-cant improvement compared with other model-freemethods and single-model baseline. However, evena simple model-free fusion can bring improvementon URFunny and MMSD2.0 datasets, indicating therobustness of our methods.

### Section 30: Section 30: 7Conclusion
Content: Content: We find that having high-quality categorized datais crucial for effective expert model training. Of-ten, unimodal information alone doesn‚Äôt provideenough useful input for predictions, leading
...
 ex-pert 
...
models to train on noisy data. This issue isparticularly pronounced with vision-based predic-tions, as discussed in Section ¬ß5. To ensure expertmodels are trained on data that reflects unique in-teraction type, we filter out any data points where|p(Yes) ‚àíp(No)| < Œ¥, with Œ¥ being a thresholdindicating the confidence of the prediction. In ex-periments with BLIP2 on URFunny, when Œ¥ = 0,meaning all training data is used, we achieve amodel-based fusion result of 73.64 F1 score. WithŒ¥ = 0.1, partial data points are included in the train-ing, and the F1 score improves to 74.65. However,when we increase Œ¥ to 0.15, the F1 score drops to73.99, likely due to the reduction in training data.Therefore, we keep Œ¥ = 0.1 for expert data filteringin our main experiments.Another technique for expert model training isto rebalance the unimodal predictions of the data to

### Section 31: Section 31: M Mo E
Content: Content: 9
...
...


### Section 32: Section 32: References
Content: Content: Juli√°n N Acosta, Guido J Falcone, Pranav Rajpurkar,and Eric J Topol. 2022. Multimodal biomedical ai.Nature Medicine, 28(9):1773‚Äì1784.Benjamin Auffarth, Maite L√≥pez, and Jes√∫s Cerquides.2010. 
...
Compariso
...
n of redundancy and relevance mea-sures for feature selection in tissue classification ofct images. In Industrial conference on data mining,pages 248‚Äì262. Springer.John Bateman. 2014. Text and image: A critical intro-duction to the visual/verbal divide. Routledge.Nils Bertschinger, Johannes Rauh, Eckehard Olbrich,J√ºrgen Jost, and Nihat Ay. 2014. Quantifying uniqueinformation. Entropy.Yitao Cai, Huiyu Cai, and Xiaojun Wan. 2019. Multi-modal sarcasm detection in Twitter with hierarchicalfusion model. In Proceedings of the 57th AnnualMeeting of the Association for Computational Lin-guistics, pages 2506‚Äì2515, Florence, Italy. Associa-tion for Computational Linguistics.While we present a first step towards classifyingand learning multimodal interactions, our catego-rization is still at a rather coarse level with onlythree interactions. Future work should investigatesub-categorizations of interactions, such as differ-ent types of synergy between modalities. This canbe used to learn mixtures of interactions at a morefine-grained feature level. Furthermore, even ap-proximate classification of interactions can leadto improved performance, so we expect future im-provements in quantifying interactions to furtherimprove MMOE. Future work can also investigatehow to better combine multiple interactions in acompositional, multi-step manner to learn morecomplex higher-order interactions between modali-ties. Finally, we only consider modalities that havehigh-quality unimodal encoders like language andvision, future work can extend this direction tonovel modalities such as sensors and medical datawhere unimodal models might have to be learnedend-to-end with the multimodal interactions.

### Section 33: Section 33: Ethics Statement
Content: Content: Santiago Castro, Devamanyu Hazarika, Ver√≥nica P√©rez-Rosas, Roger Zimmermann, Rada Mihalcea, and Sou-janya Poria. 2019. Towards multimodal sarcasm de-tection (an _Obviously_ perfect paper). In
...
 Proceed-
...
ings of the 57th Annual Meeting of the Association forComputational Linguistics, pages 4619‚Äì4629, Flo-rence, Italy. Association for Computational Linguis-tics.Minhao Cheng, Cho-Jui Hsieh, Inderjit Dhillon, et al.2020. Voting based ensemble improves robustness ofdefensive models. ArXiv preprint, abs/2011.14031.Multimodal AI systems can revolutionize manyareas involving sensing and prediction such as inmultimedia, healthcare, affective computing, andeducation, but there are also potential negative im-pacts involving monitoring and tracking humansand their states. For example, emotion detectionmodels can be used inappropriately and invade per-sonal privacy. Careful deployment to mitigate po-tential risks would be important.Ning Ding, Sheng-wei Tian, and Long Yu. 2022. A mul-timodal fusion method for sarcasm detection basedon late fusion. Multimedia Tools and Applications,81(6):8597‚Äì8616.

### Section 34: Section 34: Acknowledgement
Content: Content: AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, JakobUszkoreit, and Neil H
...
oulsby. 2
...
021.An imageis worth 16x16 words:Transformers for imagerecognition at scale. In 9th International Conferenceon Learning Representations, ICLR 2021, VirtualEvent, Austria, May 3-7, 2021. OpenReview.net.Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.2023. Palm-e: An embodied multimodal languagemodel. ArXiv preprint, abs/2303.03378.This material is based upon work partially sup-ported by National Science Foundation awards1722822 and 1750439, National Institutes ofHealth awards R01MH125740, R01MH132225,R01MH096951 and R21MH130767, and Meta.PPL is supported in part by a Siebel Scholarshipand a Waibel Presidential Fellowship. RS is sup-ported in part by ONR grant N000142312368 andDARPA FA87502321015. Any opinions, findings,conclusions, or recommendations expressed in thismaterial are those of the author(s) and do not nec-essarily reflect the views of the sponsors, and noofficial endorsement should be inferred. We thankA100 and H100 GPU support from NetMind.AI2and NVIDIA.2https://netmind.ai/homeNan Du, Yanping Huang, Andrew M. Dai, Simon Tong,Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,Yanqi Zhou, Adams Wei Yu, Orhan Firat, BarretZoph, Liam Fedus, Maarten P. Bosma, ZongweiZhou, Tao Wang, Yu Emma Wang, Kellie Webster,10Ming Hou, Jiajia Tang, Jianhai Zhang, Wanzeng Kong,and Qibin Zhao. 2019. Deep multimodal multilinearfusion with high-order polynomial pooling. In Ad-vances in Neural Information Processing Systems 32:Annual Conference on Neural Information Process-ing Systems 2019, NeurIPS 2019, December 8-14,2019, Vancouver, BC, Canada, pages 12113‚Äì12122.Marie Pellat, Kevin Robinson, Kathleen S. Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang,Quoc V. Le, Yonghui Wu, Zhifeng Chen, and ClaireCui. 2022a. Glam: Efficient scaling of languagemodels with mixture-of-experts. In InternationalConference on Machine Learning, ICML 2022, 17-23July 2022, Baltimore, Maryland, USA, volume 162 ofProceedings of Machine Learning Research, pages5547‚Äì5569. PMLR.Jan Ittner, Lukasz Bolikowski, Konstantin Hemker,and Ricardo Kennedy. 2021. Feature synergy, re-dundancy, and independence in global model ex-planations using shap vector decomposition. ArXivpreprint, abs/2107.12436.Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022b.Glm: General language model pretraining with au-toregressive blank infilling. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages320‚Äì335.David Eigen, Marc‚ÄôAurelio Ranzato, and Ilya Sutskever.2013. Learning factored representations in a deepmixture of experts. arXiv preprint arXiv:1312.4314.Andrew Jaegle, Felix Gimeno, Andy Brock, OriolVinyals, Andrew Zisserman, and Jo√£o Carreira. 2021.Perceiver: General perception with iterative attention.In Proceedings of the 38th International Conferenceon Machine Learning, ICML 2021, 18-24 July 2021,Virtual Event, volume 139 of Proceedings of MachineLearning Research, pages 4651‚Äì4664. PMLR.Ross Flom and Lorraine E Bahrick. 2007. The develop-ment of infant discrimination of affect in multimodaland unimodal stimulation: The role of intersensoryredundancy. Developmental psychology, 43(1):238.Tzyy-Ping Jung, Terrence J Sejnowski, et al. 2018.Multi-modal approach for affective computing. In2018 40th annual international conference of the ieeeengineering in medicine and biology society (embc),pages 291‚Äì294. IEEE.Yoav Freund, Robert E Schapire, et al. 1996. Experi-ments with a new boosting algorithm. In icml, vol-ume 96, pages 148‚Äì156. Citeseer.Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.2023. Grounding language models to images formultimodal inputs and outputs. In International Con-ference on Machine Learning, pages 17283‚Äì17300.PMLR.KonradGadzicki,RaziehKhamsehashari,andChristoph Zetzsche. 2020. Early vs late fusion inmultimodal convolutional neural networks. In 2020IEEE 23rd international conference on informationfusion (FUSION), pages 1‚Äì6. IEEE.Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos√© Lezama,Jonathan Huang, Rachel Hornung, Hartwig Adam,Hassan Akbari, Yair Alon, Vighnesh Birodkar,et al. 2023. Videopoet: A large language modelfor zero-shot video generation.ArXiv preprint,abs/2312.14125.Md Kamrul Hasan,Wasifur Rahman,AmirAliBagher Zadeh, Jianyuan Zhong, Md Iftekhar Tanveer,Louis-Philippe Morency, and Mohammed (Ehsan)Hoque. 2019. UR-FUNNY: A multimodal languagedataset for understanding humor. In Proceedings ofthe 2019 Conference on Empirical Methods in Natu-ral Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 2046‚Äì2056, Hong Kong,China. Association for Computational Linguistics.Julia Kruk, Jonah Lubin, Karan Sikka, Xiao Lin, DanJurafsky, and Ajay Divakaran. 2019.Integratingtext and image: Determining multimodal documentintent in Instagram posts.In Proceedings of the2019 Conference on Empirical Methods in Natu-ral Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 4622‚Äì4632, Hong Kong,China. Association for Computational Linguistics.Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Ji-dong Zhai, and Jie Tang. 2021. Fastmoe: A fastmixture-of-expert training system. ArXiv preprint,abs/2103.13262.Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large lan-guage models. ArXiv preprint, abs/2301.12597.Jack Hessel and Lillian Lee. 2020. Does my multimodalmodel learn cross-modal interactions? it‚Äôs harder totell than you might think! In Proceedings of the 2020Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 861‚Äì877, Online.Association for Computational Linguistics.Jack Hessel, Ana Marasovi¬¥c, Jena D Hwang, LillianLee, Jeff Da, Rowan Zellers, Robert Mankoff, andYejin Choi. 2022.Do androids laugh at electricsheep? humor" understanding" benchmarks fromthe new yorker caption contest.ArXiv preprint,abs/2209.06293.Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.Hoi. 2022. BLIP: bootstrapping language-image pre-training for unified vision-language understandingand generation. In International Conference on Ma-chine Learning, ICML 2022, 17-23 July 2022, Balti-more, Maryland, USA, volume 162 of Proceedingsof Machine Learning Research, pages 12888‚Äì12900.PMLR.11and Louis-Philippe Morency. 2018. Efficient low-rank multimodal fusion with modality-specific fac-tors. In Proceedings of the 56th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 2247‚Äì2256, Melbourne,Australia. Association for Computational Linguistics.Junnan Li,Ramprasaath R. Selvaraju,AkhileshGotmare, Shafiq R. Joty, Caiming Xiong, andSteven Chu-Hong Hoi. 2021.Align before fuse:Vision and language representation learning withmomentum distillation. In Advances in Neural In-formation Processing Systems 34: Annual Confer-ence on Neural Information Processing Systems 2021,NeurIPS 2021, December 6-14, 2021, virtual, pages9694‚Äì9705.Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.2019. Vilbert: Pretraining task-agnostic visiolinguis-tic representations for vision-and-language tasks. InAdvances in Neural Information Processing Systems32: Annual Conference on Neural Information Pro-cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13‚Äì23.Paul Pu Liang, Yun Cheng, Xiang Fan, Chun KaiLing, Suzanne Nie, Richard J Chen, Zihao Deng,Nicholas Allen, Randy Auerbach, Faisal Mahmood,et al. 2023a. Quantifying & modeling multimodalinteractions: An information decomposition frame-work. In Thirty-seventh Conference on Neural Infor-mation Processing Systems.Sijie Mai, Haifeng Hu, and Songlong Xing. 2019. Di-vide, conquer and combine: Hierarchical feature fu-sion network with local and global perspectives formultimodal affective computing.In Proceedingsof the 57th Annual Meeting of the Association forComputational Linguistics, pages 481‚Äì492, Florence,Italy. Association for Computational Linguistics.Paul Pu Liang, Zihao Deng, Martin Q Ma, James Zou,Louis-Philippe Morency, and Russ Salakhutdinov.2023b. Factorized contrastive learning: Going be-yond multi-view redundancy. In Thirty-seventh Con-ference on Neural Information Processing Systems.Emily E Marsh and Marilyn Domas White. 2003. Ataxonomy of relationships between images and text.Journal of documentation.Paul Pu Liang, Chun Kai Ling, Yun Cheng, AlexObolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf,Louis-Philippe Morency, and Ruslan Salakhutdinov.2023c. Multimodal learning without labeled mul-timodal data: Guarantees and applications. ArXivpreprint, abs/2306.04539.Alessio Mazzetto, Dylan Sam, Andrew Park, Eli Upfal,and Stephen H. Bach. 2021. Semi-supervised aggre-gation of dependent weak supervision sources withperformance guarantees. In The 24th InternationalConference on Artificial Intelligence and Statistics,AISTATS 2021, April 13-15, 2021, Virtual Event, vol-ume 130 of Proceedings of Machine Learning Re-search, pages 3196‚Äì3204. PMLR.Paul Pu Liang, Yiwei Lyu, Xiang Fan, Jeffrey Tsaw,Yudong Liu, Shentong Mo, Dani Yogatama, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2022.High-modality multimodal transformer: Quantify-ing modality & interaction heterogeneity for high-modality representation learning.ArXiv preprint,abs/2203.01311.Niluthpol Chowdhury Mithun, Juncheng Li, FlorianMetze, and Amit K Roy-Chowdhury. 2018. Learn-ing joint embedding with multimodal cues for cross-modal video-text retrieval. In Proceedings of the2018 ACM on international conference on multime-dia retrieval, pages 19‚Äì27.Paul Pu Liang, Amir Zadeh, and Louis-PhilippeMorency. 2023d. Foundations & trends in multi-modal machine learning: Principles, challenges, andopen questions. ACM Computing Surveys.Sarah R Partan and Peter Marler. 2005. Issues in theclassification of multimodal communication signals.The American Naturalist, 166(2):231‚Äì245.Haotian Liu, Chunyuan Li, Yuheng Li, and Yong JaeLee. 2024. Improved baselines with visual instruc-tion tuning. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition,pages 26296‚Äì26306.Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2023. Visual instruction tuning. ArXiv preprint,abs/2304.08485.Fernando P√©rez-Cruz. 2008. Estimation of informationtheoretic measures for continuous random variables.In Advances in Neural Information Processing Sys-tems 21, Proceedings of the Twenty-Second AnnualConference on Neural Information Processing Sys-tems, Vancouver, British Columbia, Canada, Decem-ber 8-11, 2008, pages 1257‚Äì1264. Curran Associates,Inc.Hui Liu, Wenya Wang, and Haoliang Li. 2022. To-wards multi-modal sarcasm detection via hierarchicalcongruity modeling with knowledge enhancement.In Proceedings of the 2022 Conference on Empiri-cal Methods in Natural Language Processing, pages4995‚Äì5006, Abu Dhabi, United Arab Emirates. As-sociation for Computational Linguistics.Libo Qin, Shijue Huang, Qiguang Chen, Chenran Cai,Yudi Zhang, Bin Liang, Wanxiang Che, and RuifengXu. 2023.MMSD2.0: Towards a reliable multi-modal sarcasm detection system.In Findings ofthe Association for Computational Linguistics: ACL2023, pages 10834‚Äì10845, Toronto, Canada. Associ-ation for Computational Linguistics.Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-narasimhan, Paul Pu Liang, AmirAli Bagher Zadeh,Natalie Ruiz, Ronnie Taib, and Fang Chen. 2006. Ex-amining the redundancy of multimodal input. In12Proceedings of the 18th Australia conference onComputer-Human Interaction: Design: Activities,Artefacts and Environments, pages 389‚Äì392.Erdal Tasci, Caner Uluturk, and Aybars Ugur. 2021. Avoting-based ensemble deep learning method focus-ing on image augmentation and preprocessing varia-tions for tuberculosis detection. Neural Computingand Applications, 33(22):15541‚Äì15555.Chitwan Saharia, William Chan, Saurabh Saxena,Lala Li, Jay Whang, Emily L Denton, Kam-yar Ghasemipour, Raphael Gontijo Lopes, BurcuKaragol Ayan, Tim Salimans, et al. 2022. Photo-realistic text-to-image diffusion models with deeplanguage understanding. Advances in neural infor-mation processing systems, 35:36479‚Äì36494.Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan,Cordelia Schmid, and Phillip Isola. 2020.Whatmakes for good views for contrastive learning? In Ad-vances in Neural Information Processing Systems 33:Annual Conference on Neural Information Process-ing Systems 2020, NeurIPS 2020, December 6-12,2020, virtual.Yuan Tian, Nan Xu, Ruike Zhang, and Wenji Mao. 2023.Sefik Serengil and Alper Ozpinar. 2024. A benchmarkof facial recognition pipelines and co-usability per-formances of modules. Journal of Information Tech-nologies, 17(2):95‚Äì107.Dynamic routing transformer network for multimodalsarcasm detection. In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 2468‚Äì2480,Toronto, Canada. Association for Computational Lin-guistics.Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,Andy Davis, Quoc V. Le, Geoffrey E. Hinton, andJeff Dean. 2017. Outrageously large neural networks:The sparsely-gated mixture-of-experts layer. In 5thInternational Conference on Learning Representa-tions, ICLR 2017, Toulon, France, April 24-26, 2017,Conference Track Proceedings. OpenReview.net.Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and fine-tuned chat models.arXiv preprintarXiv:2307.09288.Daria Sorokina, Rich Caruana, Mirek Riedewald, andDaniel Fink. 2008. Detecting statistical interactionswith additive groves of trees. In Machine Learning,Proceedings of the Twenty-Fifth International Con-ference (ICML 2008), Helsinki, Finland, June 5-9,2008, volume 307 of ACM International ConferenceProceeding Series, pages 1000‚Äì1007. ACM.Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,J. Zico Kolter, Louis-Philippe Morency, and Rus-lan Salakhutdinov. 2019. Multimodal transformerfor unaligned multimodal language sequences. InProceedings of the 57th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 6558‚Äì6569, Florence, Italy. Association for ComputationalLinguistics.Barry E Stein, Terrence R Stanford, and Benjamin ARowland. 2020. Multisensory integration and thesociety for neuroscience: Then and now. Journal ofNeuroscience, 40(1):3‚Äì11.Michael Tsang, Dehua Cheng, Hanpeng Liu, XueFeng, Eric Zhou, and Yan Liu. 2020. Feature in-teraction interpretability: A case for explaining ad-recommendation systems via neural interaction de-tection. In 8th International Conference on LearningRepresentations, ICLR 2020, Addis Ababa, Ethiopia,April 26-30, 2020. OpenReview.net.Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,Furu Wei, and Jifeng Dai. 2020. VL-BERT: pre-training of generic visual-linguistic representations.In 8th International Conference on Learning Repre-sentations, ICLR 2020, Addis Ababa, Ethiopia, April26-30, 2020. OpenReview.net.Michael Tsang, Dehua Cheng, and Yan Liu. 2018. De-tecting statistical interactions from neural networkweights. In 6th International Conference on Learn-ing Representations, ICLR 2018, Vancouver, BC,Canada, April 30 - May 3, 2018, Conference TrackProceedings. OpenReview.net.Hao Tan and Mohit Bansal. 2019. LXMERT: Learningcross-modality encoder representations from trans-formers. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages5100‚Äì5111, Hong Kong, China. Association for Com-putational Linguistics.Binghao Tang, Boda Lin, Haolong Yan, and Si Li. 2024.Peng Wang, An Yang, Rui Men, Junyang Lin, ShuaiBai, Zhikang Li, Jianxin Ma, Chang Zhou, JingrenZhou, and Hongxia Yang. 2022. OFA: unifying ar-chitectures, tasks, and modalities through a simplesequence-to-sequence learning framework. In Inter-national Conference on Machine Learning, ICML2022, 17-23 July 2022, Baltimore, Maryland, USA,volume 162 of Proceedings of Machine LearningResearch, pages 23318‚Äì23340. PMLR.Weihan Wang, Qingsong Lv, Wenmeng Yu, WenyiHong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, LeiLeveraging generative large language models with vi-sual instruction and demonstration retrieval for multi-modal sarcasm detection. In Proceedings of the 2024Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies (Volume 1: Long Papers),pages 1732‚Äì1742, Mexico City, Mexico. Associationfor Computational Linguistics.13Zhao, Xixuan Song, et al. 2023. Cogvlm: Visual ex-pert for pretrained language models. ArXiv preprint,abs/2311.03079.Ce Zhang, Taixi Lu, Md Mohaiminul Islam, ZiyangWang, Shoubin Yu, Mohit Bansal, and Gedas Berta-sius. 2023.A simple llm framework for long-range video question-answering.ArXiv preprint,abs/2312.17235.Paul L Williams and Randall D Beer. 2010.Non-negative decomposition of multivariate information.arXiv preprint arXiv:1004.2515.Mingda Zhang, Rebecca Hwa, and Adriana Kovashka.2018. Equal but not the same: Understanding theimplicit relationship between persuasive images andtext. In British Machine Vision Conference 2018,BMVC 2018, Newcastle, UK, September 3-6, 2018,page 8. BMVA Press.Thomas Winterbottom, Sarah Xiao, Alistair McLean,and Noura Al Moubayed. 2020. On modality bias inthe TVQA dataset. In 31st British Machine VisionConference 2020, BMVC 2020, Virtual Event, UK,September 7-10, 2020. BMVA Press.Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, andMohamed Elhoseiny. 2023. Minigpt-4: Enhancingvision-language understanding with advanced largelanguage models. ArXiv preprint, abs/2304.10592.Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,Aaron C. Courville, Ruslan Salakhutdinov, Richard S.Zemel, and Yoshua Bengio. 2015. Show, attend andtell: Neural image caption generation with visual at-tention. In Proceedings of the 32nd InternationalConference on Machine Learning, ICML 2015, Lille,France, 6-11 July 2015, volume 37 of JMLR Work-shop and Conference Proceedings, pages 2048‚Äì2057.JMLR.org.An Yang, Baosong Yang, Binyuan Hui, Bo Zheng,Bowen Yu, Chang Zhou, Chengpeng Li, ChengyuanLi, Dayiheng Liu, Fei Huang, et al. 2024a. Qwen2technical report. arXiv preprint arXiv:2407.10671.Dingkang Yang, Shuai Huang, Haopeng Kuang, Yang-tao Du, and Lihua Zhang. 2022. Disentangled repre-sentation learning for multimodal emotion recogni-tion. In Proceedings of the 30th ACM InternationalConference on Multimedia, pages 1642‚Äì1651.Dingkang Yang, Dongling Xiao, Ke Li, Yuzheng Wang,Zhaoyu Chen, Jinjie Wei, and Lihua Zhang. 2024b.Towards multimodal human intention understandingdebiasing via subject-deconfounding. arXiv preprintarXiv:2403.05025.Lei Yu and Huan Liu. 2003. Efficiently handling fea-ture redundancy in high-dimensional data. In Pro-ceedings of the ninth ACM SIGKDD internationalconference on Knowledge discovery and data mining.Lei Yu and Huan Liu. 2004. Efficient feature selectionvia analysis of relevance and redundancy. The Jour-nal of Machine Learning Research, 5:1205‚Äì1224.Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, andSt√©phane Deny. 2021. Barlow twins: Self-supervisedlearning via redundancy reduction. In Proceedings ofthe 38th International Conference on Machine Learn-ing, ICML 2021, 18-24 July 2021, Virtual Event,volume 139 of Proceedings of Machine LearningResearch, pages 12310‚Äì12320. PMLR.Andy Zeng, Maria Attarian, Brian Ichter, KrzysztofChoromanski, Adrian Wong, Stefan Welker, Fed-erico Tombari, Aveek Purohit, Michael Ryoo, VikasSindhwani, et al. 2022. Socratic models: Compos-ing zero-shot multimodal reasoning with language.ArXiv preprint, abs/2204.00598.14

### Section 35: Section 35: AAsset
Content: Content: widely used. While these datasets may contain in-stances of offensive content, our work does not aimto generate or amplify such content. Instead, weemploy these datasets to study and understa
...
nd thenat
...
ure of sarcasm in text. Our use of these datasetsfollows ethical guidelines, and we do not endorseor support any offensive material contained withinthem. Moreover, we have implemented measuresto mitigate the propagation of offensive contentwithin our research.

### Section 36: Section 36: BAdditional Experimental Results
Content: Content: In this section, we list all the necessary informationfor our use of models and data. In our paper, weuse MUStARD (Castro et al., 2019), URFunny (Hasanet al., 2019), MMSD2.0 (Qin et al., 2023
...
) andMMSD
...
 (Cai et al., 2019) for our dataset usage. Weuse ALBEF (Li et al., 2021), BLIP2-OPT-2.7B (Liet al., 2023), Qwen2-0.5B-Instruct (Yang et al.,2024a), Qwen2-1.5B-Instruct, Qwen2-7B-Instruct,Qwen2-72B-Instruct, CogVLM2-LLaMA3-chat-19B (Wang et al., 2023) for our model usage. Weshow the required information about them and howwe follow their requirements when using them.

### Section 37: Section 37: A.1Model and Data License
Content: Content: Besides the models listed in our main sections, wetest under more experimental settings with moremodels. Additionally, we include more baselinesfor comparison. We also include metrics of prec
...
i-sion an
...
d recall besides F1 and accuracy that havealready been included in the main section. Table 6shows comprehensive experimental results on allthe settings that we run and compare.

### Section 38: Section 38: B.1Model Details
Content: Content: ALBEF (download link)License: BSD 3-Clause "New" or "Revised"BLIP2-OPT-2.7B (download link)License: BSD 3-Clause "New" or "Revised"Qwen2-0.5B-Instruct (download link)License: Apache 2.0Qwen2-
...
1.5B-Inst
...
ruct (download link)License: Apache 2.0Qwen2-7B-Instruct (download link)License: Apache 2.0Qwen2-72B-Instruct (download link)License: Apache 2.0CogVLM2-LLaMA3-chat-19B (download link)License: Apache 2.0

### Section 39: Section 39: Offensive content claim
Content: Content: Statistical information for the splits of 4 multi-modal datasets included in our experiments is15Table 5: Statistical information for 4 multimodaldatasets that we use in our experiments. ‚Ä† in
...
dicatesth
...
at the validation split is not provided in the originaldataset and is conducted by randomly sampling fromtraining data by ourselves.Dataset#Train#Valid#Test

### Section 40: Section 40: DDataset Preprocessing Details
Content: Content: MUStARD251‚Ä†83‚Ä†356MMSD29,0402,4102,409MMSD2.019,8162,4102,409URFunny7,614980992shown in Table 5. We introduce the basic infor-mation for each dataset in the following.Different multimodal data
...
sets requ
...
ire different pre-processing methods before conducting model train-ing. We include the details of our preprocessing inthis section.

### Section 41: Section 41: MMSD
Content: Content: To allow the applicability of our method to puretext-based LLMs, we convert each image into de-tailed descriptions that include task-related infor-mation. We include the details about the pro
...
cessof us
...
ing CogVLM2-LLaMA3-chat-19B to achievethis in Table 7, 8, and 9.

### Section 42: Section 42: MMSD2.0
Content: Content: To achieve the dataset categorization based on threetypes of multimodal interaction including redun-dancy, uniqueness, and synergy. We need to finishthis in multiple steps: (1) vision-based p
...
rediction
...
collection (2) text-based prediction collection (3)multimodal data categorization. In the following16section, we include the technical details for each ofthem.

### Section 43: Section 43: F.1Vision-based Prediction Collection
Content: Content: below 0.55. These low-confidence predictions areseen as lacking clear patterns and could introducenoise into the training process. By filtering outthese training data points, we aim to improv
...
e theover
...
all quality and accuracy of the model‚Äôs predic-tions.We utilize CogVLM2-LLaMA3-chat-19B as ourbase model vision-only prediction collection. Typi-cally, even though CogVLM2-LLaMA3-chat-19Bis a multimodal model, we only include image-side information and only add task-related querieslike "Is the image sarcastic or not?" as the inputto make sure the model does not utilize text-sideinformation from the multimodal dataset to do theprediction. We conduct few-shot prompting on thetrain and validation split of all multimodal models.Since MMSD and MMSD2.0 share the same set of im-ages and conduct the same multimodal predictiontask, we show three prompts that are used for 4multimodal datasets in Table 10, 11, and 12.

### Section 44: Section 44: Rebalancing
Content: Content: We utilize Qwen2-72B-Instruct as our base modelfor text-only prediction. Typically, we only includetext-side information and task-related queries like"Is this image sarcastic or not?" as the 
...
input tom
...
ake multimodal predictions. Even though MMSDand MMSD2.0 do not share the same setting, mostof their data is similar. Therefore, we utilize thesame prompts for them. We show three few-shotprompts that are used for 4 multimodal datasets inTable 13, 14, 15.

### Section 45: Section 45: return
Content: Content: After collecting unimodal predictions for all multi-modal datasets, we conduct our algorithm for cate-gorizing each data point into different multimodalinteraction types (redundancy, uniquene
...
ss, and s
...
yn-ergy) to make sure our categorized data is suitablefor training. Typically, to achieve more robust andeffective data categorization, we design filteringand rebalancing stages as part of categorization.

### Section 46: Section 46: Filtering
Content: Content: To improve expert training, we find that instead ofstarting from the initial pre-trained model check-17the maximum sequence length to 512, rank to 16,scaling factor to 32, and dropout rate to
...
 0.05.For
...
 baseline training, the specific hyperparame-ters are as follows:

### Section 47: Section 47: For MUStARD:
Content: Content: point, it‚Äôs more effective to initialize the experttraining phase using fine-tuned baseline models.This approach leads to faster training and betteroverall results. The reasoning behind this 
...
deci-sion
...
 is that continuing training from an alreadyfine-tuned model allows the model to build on itslearned features while still maintaining strong per-formance across the entire dataset. Preserving thiscapability is essential during inference because thefusion process might assign incorrect nodes to thewrong expert models, and maintaining some gen-eral competency helps mitigate such errors fromthe fusion model and achieve better general perfor-mance.

### Section 48: Section 48: For MMSD2.0:
Content: Content: To conduct a model-based fusion, we need to traina fusion model. We use BLIP2 for classifyingmultimodal interactions and focus on three keycategories: redundancy, uniqueness, and synergy.Howe
...
ver, thes
...
e categories are often imbalanced indatasets such as MUStARD, URFunny, and MMSD2.0,with certain types being underrepresented. To ad-dress this imbalance problem, we adopt focal lossas the optimization target:FL(pt) = ‚àíŒ±t(1 ‚àípt)Œ≥ log(pt)where we set Œ± = 1 and Œ≥ = 2.For expert model training, we increase the num-ber of epochs to 10, while keeping the other hyper-parameters unchanged, to ensure sufficient training.For fusion model training, the hyperparametersvary across datasets when training BLIP2 on them:

### Section 49: Section 49: For MUStARD:
Content: Content: We include all the technical details of our experi-ments including computational requirements andhyper-parameter settings.
...
...


### Section 50: Section 50: I.3Model Selection Details
Content: Content: We utilize 5√óA6000 or 1√óA100 to run baselineexperiments. Expert model training approximatelyrequires 1.5 times longer than baseline trainingsince we need to train redundancy, uniqueness, ands
...
ynergy mo
...
dels separately. The fusion model train-ing includes a similar training configuration withbaselines but just trains under a 3-class classifica-tion.

### Section 51: Section 51: I.2Hyper-parameter Settings
Content: Content: In our experiments, which include baseline training,expert model training, and fusion model training,we consistently use the F1 score on the validationset as the metric for model selection. F
...
or baseli
...
netraining, we select the model checkpoint with thehighest F1 score on the entire development set.During expert model training, we choose the bestexpert model checkpoint based on the highest F1score on the specific subset of the development setWe use different sets of hyperparameters for the var-ious training settings, including baseline training,expert model training, and fusion model training.We do not perform hyperparameter searches butinstead tune the parameters based on the validationset. For LoRA-based fine-tuning, we generally set18that corresponds to the relevant type of multimodalinteraction. For fusion model training, we selectthe model that has the highest 3-class F1 score.

### Section 52: Section 52: I.4Evaluation Details
Content: Content: Weusedthemetricsmodulefromthescikit-learnpackageforevaluatingourprediction tasks. Since our tasks are binary predic-tion tasks, we chose the binary averaging strategyfor precision, recall, an
...
d f1. Add
...
itional details canbe found in the scikit-learn documentation forthe metrics module.

### Section 53: Section 53: I.5Experimental Statistics
Content: Content: All the available results are based on three differentrandom seeds, with both the mean and standarddeviation reported. Typically, F1 results whereadding MMOE leads to a statistically signific
...
antchange
...
 (p-value < 0.05) are marked with a ‚àóinTable 6. F1 results have a p-value < 0.1 and aremarked with a ‚àó‚àóin Table 6.

### Section 54: Section 54: JAI Assistance
Content: Content: We did use ChatGPT as the writing assistant tohelp us write part of the paper. Additionally, weutilize the power of CodePilot to help us code faster.However, all the AI-generated writing and 
...
codingcom
...
ponents assisted by AI are manually checkedand modified. There is no full AI-generated contentin the paper.19Table 6: Comprehensive results on all types of models and different datasets. The numbers in the table representthe mean values from 3 runs with 3 seeds, with the corresponding standard variance provided. ‚Ä† indicates that theresults include information from audio modality while ours does not.

### Section 55: Section 55: ModelAccPrecisionRecallF1
Content: Content: MUStARDMulT‚Ä† (Tsai et al., 2019)-65.5164.7864.49LMF‚Ä† (Liu et al., 2018)-70.4670.3469.92LF-DNN-v2‚Ä† (Ding et al., 2022)-65.9563.8862.30LMF (Liu et al., 2018)-70.7370.9070.68LF-DNN-v1‚Ä† (Ding et 
...
al., 2022
...
)-71.5571.5270.99ALBEF54.49¬±3.1347.08¬±3.0350.22¬±3.6248.51¬±2.21ALBEF+MMOE54.49¬±2.8547.36¬±2.7257.68¬±4.7651.95¬±2.81BLIP253.75¬±9.3348.46¬±4.9490.13¬±9.2162.65¬±2.67BLIP2+MMOE59.18¬±2.1151.26¬±1.3887.94¬±6.1164.74¬±2.49Qwen2-0.5B54.59¬±4.3548.35¬±3.3174.12¬±9.4058.17¬±0.86Qwen2-0.5B+MMoE49.06¬±3.0045.16¬±1.3988.60¬±3.9759.77‚àó‚àó¬±0.35Qwen2-1.5B64.79¬±4.1156.45¬±3.5378.73¬±13.1865.38¬±5.16Qwen2-1.5B+MMoE70.69¬±3.2860.86¬±3.5889.47¬±3.2972.34¬±1.50Qwen2-7B72.75¬±0.7463.27¬±1.5686.62¬±4.3872.91¬±0.74Qwen2-7B+MMoE70.41¬±3.2360.64¬±3.5789.04¬±3.3871.78¬±1.47URFunnyMulT‚Ä† (Tsai et al., 2019)66.65---FDMER (Yang et al., 2022)70.43---MMIM+SuCI‚Ä† (Yang et al., 2024b)70.92---FDMER‚Ä† (Yang et al., 2022)71.87---ALBEF66.77¬±0.2464.29¬±1.0873.74¬±2.9068.67¬±0.79ALBEF+MMoE67.91¬±0.2765.17¬±0.3075.24¬±1.5369.85‚àó¬±0.52BLIP270.43¬±0.2065.14¬±0.2386.60¬±1.0774.31¬±0.35BLIP2+MMoE71.27¬±0.3066.60¬±1.2384.15¬±1.9574.32¬±0.36Qwen2-0.5B69.29¬±0.5467.16¬±1.7074.15¬±1.8570.46¬±0.14Qwen2-0.5B+MMoE69.19¬±0.1469.36¬±0.0767.55¬±0.4468.38¬±0.20Qwen2-1.5B70.43¬±0.5366.03¬±0.4183.13¬±2.2773.51¬±0.89Qwen2-1.5B+MMoE68.25¬±0.8164.40¬±1.8780.07¬±1.3771.34¬±0.52Qwen2-7B72.41¬±0.5268.14¬±0.6582.93¬±0.4374.80¬±0.55Qwen2-7B+MMoE71.88¬±0.5169.18¬±0.6778.16¬±2.5673.29¬±0.86MMSD2.0HKE (Liu et al., 2022)76.5073.4871.0772.25ViT (Dosovitskiy et al., 2021)72.0265.2674.8369.72DynRT-Net (Tian et al., 2023)71.4071.8072.1771.34Multi-view CLIP (Qin et al., 2023)85.6480.3388.2484.10ChatGLM2-6B (Du et al., 2022b)80.0880.5281.0480.04LLaMA2-7B (Touvron et al., 2023)84.6884.4084.9484.53LLaVA1.5-7B (Liu et al., 2024)85.1885.8985.2085.11LLaVA1.5-7B+DemoRetrieval (Tang et al., 2024)86.4387.0086.3086.34ALBEF81.79¬±0.8677.58¬±1.3581.23¬±1.5979.33¬±0.18ALBEF+MMOE82.30¬±0.3176.24¬±0.2485.57¬±0.4280.63‚àó¬±0.32BLIP284.75¬±0.9978.08¬±1.6589.78¬±2.7183.52¬±0.04BLIP2+MMOE84.82¬±0.8778.87¬±1.6088.49¬±2.3683.38¬±0.05Qwen2-0.5B81.87¬±0.8175.83¬±1.4785.09¬±1.5980.17¬±0.14Qwen2-0.5B+MMoE82.27¬±0.6476.02¬±1.3785.92¬±4.0780.67‚àó¬±1.55Qwen2-1.5B83.24¬±1.3278.81¬±2.3383.54¬±4.5181.10¬±0.94Qwen2-1.5B+MMoE82.76¬±0.6376.70¬±1.2886.21¬±3.3781.16¬±0.76Qwen2-7B85.28¬±1.1780.38¬±0.8787.05¬±2.4383.58¬±1.29Qwen2-7B+MMoE84.35¬±1.0578.74¬±2.6587.21¬±4.3482.74¬±0.4320Table 7: Prompt for generating image description of MUStARD

### Section 56: Section 56: RoleContent
Content: Content: SystemDescribe the image in detail.If there are people, focus on their emotions, postures, facialexpressions,bodylanguage,andinteractions.Basedonthisinformation, infer what event is going on.
...
If there 
...
are no people, analyze the event or scene, consideringbackground elements and overall context to infer what event is goingon.Provide evidence to predict if the situation is humorous.Ensure the description is between 15 to 100 words.Table 8: Prompt for generating image description of MMSD and MMSD2.0

### Section 57: Section 57: RoleContent
Content: Content: SystemDescribe the image in detail.If there are people, focus on their emotions, postures, facialexpressions,bodylanguage,andinteractions.Basedonthisinformation, infer what is the event going
...
 on.If th
...
ere are no people, analyze the event or scene, consideringbackground elements and overall context to infer what is the eventgoing on.Provide evidence to predict if the situation is sarcastic.Ensure the description is between 15 to 100 words.Table 9: Prompt for generating image description of URFunny

### Section 58: Section 58: RoleContent
Content: Content: SystemDescribe the image in detail.If there are people, focus on their emotions, postures, facialexpressions,bodylanguage,andinteractions.Basedonthisinformation, infer what is the event going
...
 on.If th
...
ere are no people, analyze the event or scene, consideringbackground elements and overall context to infer what is the eventgoing on.Provide evidence to predict if the situation is sarcastic.Ensure the description is between 15 to 100 words.21Table 10: Prompt for generating image-only prediction of MUStARD

### Section 59: Section 59: RoleContent
Content: Content: SystemPlease analyze the image provided for sarcastic or not. The imageis a screenshot of a TV show.If you think the image includes exaggerated emotions (like laughingor looking angry or rais
...
ing eyebr
...
ows) or exaggerated posture (likestretching hands), please answer ‚ÄôYes‚Äô.If you think the image shows people discussing serious things andjust daily routines, please answer ‚ÄôNo‚Äô.You need to think about what is the potential event going on in theimage.Please make sure that your answer is based on the image itself, noton the context or your knowledge.There are only two options: ‚ÄôYes‚Äô or ‚ÄôNo‚Äô.If you are not sure, please provide your best guess and do not saythat you are not sure.You should only make No judgment when you are very sure that theimage is not sarcastic.As long as you think potentially it issarcastic, you should say Yes.Table 11: Prompt for generating image-only prediction of MMSD and MMSD2.0

### Section 60: Section 60: RoleContent
Content: Content: SystemPlease analyze the image provided for sarcastic or not. The image isa screenshot of the image on Twitter. It might include a lot of text,so you need to combine the information of the te
...
xt in the
...
 image.If you think the image includes exaggerated emotions (like laughingor looking angry or raising eyebrows) or exaggerated posture (likestretching hands), please answer ‚ÄôYes‚Äô.If you think the image includes text that is sarcastic or exaggerated,please answer ‚ÄôYes‚Äô.If you think the image shows people discussing serious things andjust daily routines, please answer ‚ÄôNo‚Äô.You need to think about what is the potential event going on in theimage.Please make sure that your answer is based on the image itself, noton the context or your knowledge.There are only two options: ‚ÄôYes‚Äô or ‚ÄôNo‚Äô.If you are not sure, please provide your best guess and do not saythat you are not sure.You should only make No judgment when you are very sure that the textis not sarcastic. As long as you think potentially it is sarcastic,you should say Yes.22Table 12: Prompt for generating image-only prediction of URFunny

### Section 61: Section 61: RoleContent
Content: Content: SystemYou are looking at a screenshot of a TED talk. It is part of thetalk and it can be a slide or a speaker.Please analyze the image provided to show whether the image is partof a talk that
...
 is showi
...
ng serious content or trying to show somepotentially funny content that can make the audience laugh.If you are looking at a slide, please think about the content of theslide.If the slide is showing some very interesting and informal things,we believe the speaker is trying to make some jokes, and pleaseanswer ‚ÄôYes‚Äô.If the slide is showing some very serious and formal things, webelieve the speaker is trying to show some serious content andplease answer ‚ÄôNo‚Äô.If you are looking at a speaker, please think about the speaker‚Äôsfacial expression and body language.If you think the image includes exaggerated emotions or its bodylanguage is exaggerated, we believe the speaker is talking aboutsome informal things and please answer ‚ÄôYes‚Äô.If you think the speaker in the image looks very serious and formal,they are trying to convey their key points and please answer ‚ÄôNo‚Äô.Please make sure that your answer is based on the image itself, noton the context or your knowledge.There are only two options: ‚ÄôYes‚Äô or ‚ÄôNo‚Äô.If you are not sure, please provide your best guess and do not saythat you are not sure.23Table 13: Prompt for generating text-only prediction of MUStARD

### Section 62: Section 62: RoleContent
Content: Content: SystemPlease analyze the text provided below for sarcasm.If you think the text includes an exaggerated description or includesstrong emotion or its real meaning is not aligned with the origin
...
alone, pl
...
ease answer ‚ÄôYes‚Äô.If you think the text is neutral or its true meaning is not differentfrom its original one, please answer ‚ÄôNo‚Äô.Please make sure that your answer is based on the text itself, noton the context or your knowledge.There are only two options: ‚ÄôYes‚Äô or ‚ÄôNo‚Äô.If you are not sure, please provide your best guess and do not saythat you are not sure.You should only make Yes judgment when you are very sure that thetext is sarcastic.UserTEXT: Yes yes it is! In Prison!!Assistant Yes. It expresses the speaker‚Äôs strong emotion about the situationwhich indicates that the speaker is sarcastic.UserTEXT: And then and then you clicked it again, she‚Äôs dressed. She isa businesswoman, she is walking down the street and oh oh oh she‚Äôsnaked.Assistant No. It is a neutral statement.Table 14: Prompt for generating text-only prediction of MMSD and MMSD2.0

### Section 63: Section 63: RoleContent
Content: Content: SystemPlease analyze the text provided below for sarcasm.If you think the text includes an exaggerated description or itsreal meaning is not aligned with the original one, please answer‚ÄôYes‚Äô.
...
If you th
...
ink the text is neutral or its true meaning is not differentfrom its original one, please answer ‚ÄôNo‚Äô.Please make sure that your answer is based on the text itself, noton the context or your knowledge.There are only two options: ‚ÄôYes‚Äô or ‚ÄôNo‚Äô.If you are not sure, please provide your best guess and do not saythat you are not sure.UserTEXT: because lunch is more interesting than job and even tasty...Assistant Yes. It expresses the speaker‚Äôs preference for lunch over the jobby using the word ‚Äôtasty‚Äô.UserTEXT: gameday ready‚ÄôAssistant No. It is a neutral statement.24Table 15: Prompt for generating text-only prediction of URFunny

### Section 64: Section 64: RoleContent
Content: Content: SystemPlease analyze the text provided below for humor or not.If you think the text includes an exaggerated description or it isexpressing sarcastic meaning, please answer ‚ÄôYes‚Äô.If you think 
...
the text 
...
is neutral or just common meaning, pleaseanswer ‚ÄôNo‚Äô.Please make sure that your answer is based on the text itself, noton the context or your knowledge.There are only two options: ‚ÄôYes‚Äô or ‚ÄôNo‚Äô.If you are not sure, please provide your best guess and do not saythat you are not sure.You should only make No judgment when you are very sure that thetext is not funny. As long as you think potentially it is funny, youshould say Yes.UserTEXT: why invite men they are the problemAssistant Yes. It expresses that men can be problematic and the speaker issarcastic to make people laugh.UserTEXT: we all feel the same things.Assistant No. It is a neutral statement.25

